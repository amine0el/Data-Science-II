{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://i.imgur.com/LZB5LEa.png)\n\n# Introduction\n\n### Why are we doing this?\nMusic. Experts have been trying for a long time to understand sound and what differenciates one song from another. How to visualize sound. What makes a tone different from another.\n\nIn this notebook we will go through an in depth analysis of sound and how we can **visualize, classify and ultimately understand** it.\n\n### The data:\n* **genres original** - A collection of 10 genres with 100 audio files each, all having a length of 30 seconds (the famous GTZAN dataset, the MNIST of sounds)\n* **images original** - A visual representation for each audio file. One way to classify data is through neural networks. Because NNs (like CNN, what we will be using today) usually take in some sort of image representation, the audio files were converted to Mel Spectrograms to make this possible (we'll be talking about this more in depth later)\n* **2 CSV files** - Containing features of the audio files. One file has for each song (30 seconds long) a mean and variance computed over multiple features that can be extracted from an audio file (more in depth later). The other file has the same structure, but the songs were split before into 3 seconds audio files (this way increasing 10 times the amount of data we fuel into our classification models). *With data, more is always better*.\n\n### Purpose:\n1. We want to understand **what is an Audio file**. What features we can visualize on this kind of data.\n2. **EDA**. ALways good, here very necessary.\n3. **Genres Classification** on the 3 seconds CSV file (trying multiple models and identify which has the best accuracy)\n4. A **recommender system**: given a song, give me top X songs most similar.\n\n### Credits and Acknowledgements:\n* This was a team project for uni, so the effort in creating this wasn't only my own. So, I want to thank **James Wiltshire, Lauren O'Hare and Minyu Lei** for being the best teammates ever and for having so much fun and learning so much during the 3 days we worked on this.\n* You can do a lot of documentation online, but an article that helped us a lot in doing this was [Parul Pandey's](https://www.kaggle.com/parulpandey) notebook for [Music Genre Classification with Python](https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8)","metadata":{}},{"cell_type":"code","source":"# Usual Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sklearn\n\n# Librosa (the mother of audio files)\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ngeneral_path = '../input/gtzan-dataset-music-genre-classification/Data'\nprint(list(os.listdir(f'{general_path}/genres_original/')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore Audio Data\n\nWe will use `librosa`, which is the mother of audio files.\n\n## Understanding Audio\nLet's first Explore our Audio Data to see how it looks (we'll work with `reggae.00036.wav` file).\n\n* **Sound**: sequence of vibrations in varying pressure strengths (`y`)\n* The **sample rate** (`sr`) is the number of samples of audio carried per second, measured in Hz or kHz","metadata":{}},{"cell_type":"code","source":"# Importing 1 file\ny, sr = librosa.load(f'{general_path}/genres_original/reggae/reggae.00036.wav')\n\nprint('y:', y, '\\n')\nprint('y shape:', np.shape(y), '\\n')\nprint('Sample Rate (KHz):', sr, '\\n')\n\n# Verify length of the audio\nprint('Check Len of Audio:', 661794/22050)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trim leading and trailing silence from an audio signal (silence before and after the actual audio)\naudio_file, _ = librosa.effects.trim(y)\n\n# the result is an numpy ndarray\nprint('Audio File:', audio_file, '\\n')\nprint('Audio File shape:', np.shape(audio_file))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2D Representation: Sound Waves","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(y = audio_file, sr = sr, color = \"#A300F9\");\nplt.title(\"Sound Waves in Reggae 36\", fontsize = 23);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fourier Transform\n\n* Function that gets a signal in the time domain as input, and outputs its decomposition into frequencies\n* Transform both the y-axis (frequency) to log scale, and the “color” axis (amplitude) to Decibels, which is approx. the log scale of amplitudes.","metadata":{}},{"cell_type":"code","source":"# Default FFT window size\nn_fft = 2048 # FFT window size\nhop_length = 512 # number audio of frames between STFT columns (looks like a good default)\n\n# Short-time Fourier transform (STFT)\nD = np.abs(librosa.stft(audio_file, n_fft = n_fft, hop_length = hop_length))\n\nprint('Shape of D object:', np.shape(D))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (16, 6))\nplt.plot(D);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Spectrogram\n\n* What is a spectrogram? A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. When applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams ([wiki](https://en.wikipedia.org/wiki/Spectrogram)).\n* Here we convert the frequency axis to a logarithmic one.","metadata":{}},{"cell_type":"code","source":"# Convert an amplitude spectrogram to Decibels-scaled spectrogram.\nDB = librosa.amplitude_to_db(D, ref = np.max)\n\n# Creating the Spectogram\nplt.figure(figsize = (16, 6))\nlibrosa.display.specshow(DB, sr = sr, hop_length = hop_length, x_axis = 'time', y_axis = 'log',\n                        cmap = 'cool')\nplt.colorbar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mel Spectrogram\n\n* The Mel Scale, mathematically speaking, is the result of some non-linear transformation of the frequency scale. The Mel Spectrogram is a normal Spectrogram, but with a Mel Scale on the y axis.","metadata":{}},{"cell_type":"code","source":"y, sr = librosa.load(f'{general_path}/genres_original/metal/metal.00036.wav')\ny, _ = librosa.effects.trim(y)\n\n\nS = librosa.feature.melspectrogram(y, sr=sr)\nS_DB = librosa.amplitude_to_db(S, ref=np.max)\nplt.figure(figsize = (16, 6))\nlibrosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis = 'time', y_axis = 'log',\n                        cmap = 'cool');\nplt.colorbar();\nplt.title(\"Metal Mel Spectrogram\", fontsize = 23);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y, sr = librosa.load(f'{general_path}/genres_original/classical/classical.00036.wav')\ny, _ = librosa.effects.trim(y)\n\n\nS = librosa.feature.melspectrogram(y, sr=sr)\nS_DB = librosa.amplitude_to_db(S, ref=np.max)\nplt.figure(figsize = (16, 6))\nlibrosa.display.specshow(S_DB, sr=sr, hop_length=hop_length, x_axis = 'time', y_axis = 'log',\n                        cmap = 'cool');\nplt.colorbar();\nplt.title(\"Classical Mel Spectrogram\", fontsize = 23);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Audio Features\n\n### Zero Crossing Rate\n\n* the rate at which the signal changes from positive to negative or back.","metadata":{}},{"cell_type":"code","source":"# Total zero_crossings in our 1 song\nzero_crossings = librosa.zero_crossings(audio_file, pad=False)\nprint(sum(zero_crossings))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Harmonics and Perceptrual\n\n* Harmonics are characteristichs that human years can't distinguish (represents the sound color)\n* Perceptrual understanding shock wave represents the sound rhythm and emotion","metadata":{}},{"cell_type":"code","source":"y_harm, y_perc = librosa.effects.hpss(audio_file)\n\nplt.figure(figsize = (16, 6))\nplt.plot(y_harm, color = '#A300F9');\nplt.plot(y_perc, color = '#FFB100');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tempo BMP (beats per minute)\n\nDynamic programming beat tracker.","metadata":{}},{"cell_type":"code","source":"tempo, _ = librosa.beat.beat_track(y, sr = sr)\ntempo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spectral Centroid\n\n* indicates where the ”centre of mass” for a sound is located and is calculated as the weighted mean of the frequencies present in the sound.","metadata":{}},{"cell_type":"code","source":"# Calculate the Spectral Centroids\nspectral_centroids = librosa.feature.spectral_centroid(audio_file, sr=sr)[0]\n\n# Shape is a vector\nprint('Centroids:', spectral_centroids, '\\n')\nprint('Shape of Spectral Centroids:', spectral_centroids.shape, '\\n')\n\n# Computing the time variable for visualization\nframes = range(len(spectral_centroids))\n\n# Converts frame counts to time (seconds)\nt = librosa.frames_to_time(frames)\n\nprint('frames:', frames, '\\n')\nprint('t:', t)\n\n# Function that normalizes the Sound Data\ndef normalize(x, axis=0):\n    return sklearn.preprocessing.minmax_scale(x, axis=axis)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting the Spectral Centroid along the waveform\nplt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(audio_file, sr=sr, alpha=0.4, color = '#A300F9');\nplt.plot(t, normalize(spectral_centroids), color='#FFB100');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spectral Rolloff\n* is a measure of the shape of the signal. It represents the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies","metadata":{}},{"cell_type":"code","source":"# Spectral RollOff Vector\nspectral_rolloff = librosa.feature.spectral_rolloff(audio_file, sr=sr)[0]\n\n# The plot\nplt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(audio_file, sr=sr, alpha=0.4, color = '#A300F9');\nplt.plot(t, normalize(spectral_rolloff), color='#FFB100');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mel-Frequency Cepstral Coefficients:\n\n* The Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope. It models the characteristics of the human voice.","metadata":{}},{"cell_type":"code","source":"mfccs = librosa.feature.mfcc(audio_file, sr=sr)\nprint('mfccs shape:', mfccs.shape)\n\n#Displaying  the MFCCs:\nplt.figure(figsize = (16, 6))\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap = 'cool');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data needs to be scaled:","metadata":{}},{"cell_type":"code","source":"# Perform Feature Scaling\nmfccs = sklearn.preprocessing.scale(mfccs, axis=1)\nprint('Mean:', mfccs.mean(), '\\n')\nprint('Var:', mfccs.var())\n\nplt.figure(figsize = (16, 6))\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time', cmap = 'cool');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chroma Frequencies\n\n* Chroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma) of the musical octave.","metadata":{}},{"cell_type":"code","source":"# Increase or decrease hop_length to change how granular you want your data to be\nhop_length = 5000\n\n# Chromogram\nchromagram = librosa.feature.chroma_stft(audio_file, sr=sr, hop_length=hop_length)\nprint('Chromogram shape:', chromagram.shape)\n\nplt.figure(figsize=(16, 6))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nEDA is going to be performed on the `features_30_sec.csv`. This file contains the mean and variance for each audio file fo the features analysed above. \n\nSo, the table has a final of 1000 rows (10 genrex x 100 audio files) and 60 features (dimensionalities).","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(f'{general_path}/features_30_sec.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Heatmap for feature means","metadata":{}},{"cell_type":"code","source":"# Computing the Correlation Matrix\nspike_cols = [col for col in data.columns if 'mean' in col]\ncorr = data[spike_cols].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 11));\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 25, as_cmap=True, s = 90, l = 45, n = 5)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.title('Correlation Heatmap (for the MEAN variables)', fontsize = 25)\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10);\nplt.savefig(\"Corr Heatmap.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Box Plot for Genres Distributions","metadata":{}},{"cell_type":"code","source":"x = data[[\"label\", \"tempo\"]]\n\nf, ax = plt.subplots(figsize=(16, 9));\nsns.boxplot(x = \"label\", y = \"tempo\", data = x, palette = 'husl');\n\nplt.title('BPM Boxplot for Genres', fontsize = 25)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 10);\nplt.xlabel(\"Genre\", fontsize = 15)\nplt.ylabel(\"BPM\", fontsize = 15)\nplt.savefig(\"BPM Boxplot.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Principal Component Analysis - to visualize possible groups of genres\n\n1. Normalization\n2. PCA\n3. The Scatter Plot","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\ndata = data.iloc[0:, 1:]\ny = data['label']\nX = data.loc[:, data.columns != 'label']\n\n#### NORMALIZE X ####\ncols = X.columns\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X)\nX = pd.DataFrame(np_scaled, columns = cols)\n\n\n#### PCA 2 COMPONENTS ####\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n\n# concatenate with target label\nfinalDf = pd.concat([principalDf, y], axis = 1)\n\npca.explained_variance_ratio_\n\n# 44.93 variance explained","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (16, 9))\nsns.scatterplot(x = \"principal component 1\", y = \"principal component 2\", data = finalDf, hue = \"label\", alpha = 0.7,\n               s = 100);\n\nplt.title('PCA on Genres', fontsize = 25)\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 10);\nplt.xlabel(\"Principal Component 1\", fontsize = 15)\nplt.ylabel(\"Principal Component 2\", fontsize = 15)\nplt.savefig(\"PCA Scattert.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Classification\n\nUsing the `features_3_sec.csv` file, we can try to build a classifier that accurately predicts for any new audio file input it's genre.\n\n### Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading in the Data\n\nNow let's try to predict the Genre of the audio using Machine Learning techniques.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(f'{general_path}/features_3_sec.csv')\ndata = data.iloc[0:, 1:] \ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features and Target variable\n\n* creates the target and feature variables\n* normalizes the data","metadata":{}},{"cell_type":"code","source":"y = data['label'] # genre variable.\nX = data.loc[:, data.columns != 'label'] #select all columns but not the labels\n\n#### NORMALIZE X ####\n\n# Normalize so everything is on the same scale. \n\ncols = X.columns\nmin_max_scaler = preprocessing.MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(X)\n\n# new data frame with the new scaled data. \nX = pd.DataFrame(np_scaled, columns = cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the data into training and testing\n\n* 70% - 30% split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating a Predefined function to assess the accuracy of a model\n\n* input is the model\n* fits the model on the training dataset\n* predicts on the testing features\n* compares the predictions with the actuals","metadata":{}},{"cell_type":"code","source":"def model_assess(model, title = \"Default\"):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    #print(confusion_matrix(y_test, preds))\n    print('Accuracy', title, ':', round(accuracy_score(y_test, preds), 5), '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trying 10 different models to assess their performance\n\nWe tried 10 classification models, the best performing model was XGBooster.","metadata":{}},{"cell_type":"code","source":"# Naive Bayes\nnb = GaussianNB()\nmodel_assess(nb, \"Naive Bayes\")\n\n# Stochastic Gradient Descent\nsgd = SGDClassifier(max_iter=5000, random_state=0)\nmodel_assess(sgd, \"Stochastic Gradient Descent\")\n\n# KNN\nknn = KNeighborsClassifier(n_neighbors=19)\nmodel_assess(knn, \"KNN\")\n\n# Decission trees\ntree = DecisionTreeClassifier()\nmodel_assess(tree, \"Decission trees\")\n\n# Random Forest\nrforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)\nmodel_assess(rforest, \"Random Forest\")\n\n# Support Vector Machine\nsvm = SVC(decision_function_shape=\"ovo\")\nmodel_assess(svm, \"Support Vector Machine\")\n\n# Logistic Regression\nlg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\nmodel_assess(lg, \"Logistic Regression\")\n\n# Neural Nets\nnn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)\nmodel_assess(nn, \"Neural Nets\")\n\n# Cross Gradient Booster\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\nmodel_assess(xgb, \"Cross Gradient Booster\")\n\n# Cross Gradient Booster (Random Forest)\nxgbrf = XGBRFClassifier(objective= 'multi:softmax')\nmodel_assess(xgbrf, \"Cross Gradient Booster (Random Forest)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost is the winner - 90% accuracy\n\n* create the final model\n* compute confusion matrix\n* Compute Feature Importance","metadata":{}},{"cell_type":"code","source":"# Final model\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\nxgb.fit(X_train, y_train)\n\n\npreds = xgb.predict(X_test)\n\nprint('Accuracy', ':', round(accuracy_score(y_test, preds), 5), '\\n')\n\n# Confusion Matrix\nconfusion_matr = confusion_matrix(y_test, preds) #normalize = 'true'\nplt.figure(figsize = (16, 9))\nsns.heatmap(confusion_matr, cmap=\"Blues\", annot=True, \n            xticklabels = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"],\n           yticklabels=[\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]);\nplt.savefig(\"conf matrix\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(estimator=xgb, random_state=1)\nperm.fit(X_test, y_test)\n\neli5.show_weights(estimator=perm, feature_names = X_test.columns.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recommender Systems\n\n\"Recomender\" Systems enable us for any given vector to find the best similarity, ranked in descending order, from the bast match to the least best match. \n\nFor Audio files, this will be done through `cosine_similarity` library.","metadata":{}},{"cell_type":"code","source":"# Libraries\nimport IPython.display as ipd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn import preprocessing\n\n# Read data\ndata = pd.read_csv(f'{general_path}/features_30_sec.csv', index_col='filename')\n\n# Extract labels\nlabels = data[['label']]\n\n# Drop labels from original dataframe\ndata = data.drop(columns=['length','label'])\ndata.head()\n\n# Scale the data\ndata_scaled=preprocessing.scale(data)\nprint('Scaled data type:', type(data_scaled))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cosine similarity\n\nCalculates the *pairwise cosine similarity* for each combination of songs in the data. This results in a 1000 x 1000 matrix (with redundancy in the information as item A similarity to item B == item B similarity to item A).","metadata":{}},{"cell_type":"code","source":"# Cosine similarity\nsimilarity = cosine_similarity(data_scaled)\nprint(\"Similarity shape:\", similarity.shape)\n\n# Convert into a dataframe and then set the row index and column names as labels\nsim_df_labels = pd.DataFrame(similarity)\nsim_df_names = sim_df_labels.set_index(labels.index)\nsim_df_names.columns = labels.index\n\nsim_df_names.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Song similarity scoring\n\n`find_similar_songs()` - is a predefined function that takes the name of the song and returns top 5 best matches for that song.","metadata":{}},{"cell_type":"code","source":"def find_similar_songs(name):\n    # Find songs most similar to another song\n    series = sim_df_names[name].sort_values(ascending = False)\n    \n    # Remove cosine similarity == 1 (songs will always have the best match with themselves)\n    series = series.drop(name)\n    \n    # Display the 5 top matches \n    print(\"\\n*******\\nSimilar songs to \", name)\n    print(series.head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Putting the Similarity Function into Action:\n\n### POP Example","metadata":{}},{"cell_type":"code","source":"# pop.00019 - Britney Spears \"Hit me baby one more time\"\nfind_similar_songs('pop.00019.wav') \n\nipd.Audio(f'{general_path}/genres_original/pop/pop.00019.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.1 \n*Britney Spears* - **I'm so curious (2009 remaster)**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/pop/pop.00023.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.2\n*Britney Spears* - **Sometimes**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/pop/pop.00034.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.3\n*Jennifer Lopez* - **Play**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/pop/pop.00078.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.4\n*Mandy Moore* - **Candy**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/pop/pop.00088.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.5\n*Mandy Moore* - **Everything my heart desires**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/pop/pop.00091.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### METAL Example","metadata":{}},{"cell_type":"code","source":"# metal.00002 - Iron Maiden \"Flight of Icarus\"\nfind_similar_songs('metal.00002.wav') \n\nipd.Audio(f'{general_path}/genres_original/metal/metal.00002.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.1 \n*Motorhead* - **Ace of Spades**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/metal/metal.00028.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.2\n*Queen* - **Tear it Up**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/metal/metal.00059.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.3\n*Queen* - **Another One Bites The Dust**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/rock/rock.00018.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.4\n*Queen* - **Under Pressure**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/rock/rock.00017.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar song match no.5\n*Queen* - **Tie Your Mother Down**","metadata":{}},{"cell_type":"code","source":"ipd.Audio(f'{general_path}/genres_original/rock/rock.00016.wav')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<p>If you liked this, don't be shy, upvote! 😁<p>\n\n<b>Toodles!<b>\n</div>","metadata":{}}]}